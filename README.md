# EPFL CS433 Machine Learning Project 1 - Finding the Higgs Boson
## Contributors
Weiran Wang (weiran.wang@epfl.ch)  
Yang Gao (yang.gao@epfl.ch)  
Yuan Cheng (yuan.cheng@epfl.ch)  

## Project Background & Target
The Higgs boson is an elementary particle in the Standard Model of physics which explains why other particles have mass. Its discovery at the Large Hadron Collider at CERN was announced in March 2013.  
In this project, our team applied machine learning techniques to actual CERN particle accelerator data to recreate the process of "discovering" the Higgs particle.

## Project Structure
```
.
├── README.md
├── cross_validation.py
├── implementations.py
├── preprocess.py
├── proj1_helpers.py
├── run.py
├── test.csv
├── train.csv
└── visualization.py
```

## Data and Figures
Since the files exceed the single file size restriction in Github, we upload the data and figures into the Google Drive. All the [training_set](https://drive.google.com/file/d/1ErJw8BWAFxAWSHwtS7k7prBiD3POTYeQ/view?usp=sharing) and [testing _set](https://drive.google.com/file/d/1GUiYqKvnZB5_TZnouZ31HJq30fbt8oIB/view?usp=sharing). Moreover, [submission_csv_to_AICrowd](https://drive.google.com/drive/folders/1P-khad6fGv8DxxBBq_aXFJLDS-xjXzRO?usp=sharing) which contains the predicted value -1 & 1, [visualization_csv](https://drive.google.com/drive/folders/1dsWkAK3uPTZxYBqiRwy_f7TY-0DMXfs4?usp=sharing) and [figures](https://drive.google.com/drive/folders/1DoMJ0sE8dAcLmxTX9UB87gilsqk0Nr3j?usp=sharing) can also be found in Google Drive.

## Python Scripts Description
`proj1_helpers.py`  
This script is provided by the Professors and TAs which includes loading the training/test csv files, predicting the labels via trained w and creating the csv submission files to the AICrowd.

`preprocess.py`  
This is a preprocess step for the implementations.py containing compute_loss, compute_gradient, negative_log_likelihood and so on. All the fundamental computation for implementations.py can be found here.

`implementations.py`  
This is the main python scripts required by the project. As can be seen from [project1_description.pdf](https://github.com/epfml/ML_course/blob/master/projects/project1/project1_description.pdf), six model functions are realized here. Each function will return w and loss where we will use the w to test our testing set.  
|NO. | function name   | function description |  
|:---:| :--------------- | :-------------------- |  
|1 | least_square_GD | Linear regression using gradient descent |
|2 | least_squares_SGD | Linear regression using stochastic gradient descent |
|3 | least_square | Least squares regression using normal equations |
|4 | ridge_regression | Ridge regression using normal equations |
|5 | logistic_regression | Logistic regression using gradient descent or SGD |
|6 | reg_logistic_regression | Regularized logistic regression using gradient descent|

`visualization.py`  
This is used to visualize the 'signal (1)' and 'background (-1)' results where blue points stand for background while orange for signal. We use a tool called [TSNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) recommended by one of the TAs to reduce the dimensionality from 30 to 2, and successfully cluster the two different classifications. Compared to the figure generated by trainging set, we believe we get a relative good result. It's worth mentioning that considering the tremendous size of both training set and testing sets, we select the first one million data to visualize so as to relieve the visualiztion generation burden of our computers.

`run.py`  
Our main function is implemented here where users can choose a number between 1-6 corresponding to different model functions. Other input number will give an error message to re-input a correct number.

`validation.py`  
This file shows the implementation of the cross validation by k-fold function. We first build k indices for k-fold, then design a cross validation demo with ridge regression. During the demo validation process, we catch the k'th subgroup in test dataset and catch others in train dataset, then we use polynomial degree to build our data. Finally we calculate the loss for train and test with 'w' derived by ridge regression. 

## Results from AICrowd
|NO. | function name   | accuracy |  
|:---:| :---------------: | :--------------------: |  
|1 | least_square_GD | 0.684 |
|2 | least_squares_SGD | 0.688 |
|3 | least_square | 0.745 |
|4 | ridge_regression | 0.735 |
|5 | logistic_regression | 0.663 |
|6 | reg_logistic_regression | 0.656|

## Report
The report is written in a given LaTeX format and relative figures and descriptive explanation can be found there. We are welcome everyone to email us if there is something unclear in our project. Furthermore, any ideas are also warmly welcome!

## Special Acknowledgement
Sincere appreciation to Lie He, the TA who helps us to solve our puzzles in this project, and all other professors and TAs who offer us the assistance during lab sessions.